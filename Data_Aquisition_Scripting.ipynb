{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project FriendlyFire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to Scrape Steam Data\n",
    "\n",
    "This script scrapes data from the Steam platform using the `Steam Web API`. It performs the following tasks:\n",
    "\n",
    "1. **Initial Setup**: Imports necessary libraries and sets up initial parameters such as API key, directories for saving data, and files for logging errors and storing results.\n",
    "2. **Function Definitions**: Defines various functions to handle API calls, log errors, load existing IDs, append to files, calculate friendship duration, and process player data.\n",
    "3. **Gather Steam IDs**: Collects Steam IDs up to a specified depth by traversing the friends' network starting from a given Steam ID.\n",
    "4. **Process Players in Batches**: Processes the collected Steam IDs in batches using multithreading to retrieve player summaries, owned games, and friends' data.\n",
    "5. **Save Results**: Saves the collected data in JSON format and logs any errors encountered during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from steam.webapi import WebAPI\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "#### INITIAL PARAMETERS ####\n",
    "\n",
    "API_KEY = \"\"\n",
    "api = WebAPI(key=API_KEY)\n",
    "data_dir = \"Steam_Scraped_Data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "error_log_file = os.path.join(data_dir, \"error_log.txt\")\n",
    "output_csv = os.path.join(data_dir, \"all_steam_data.csv\")\n",
    "output_json = os.path.join(data_dir, \"all_steam_data.json\")\n",
    "\n",
    "successful_file = os.path.join(data_dir, \"successful_ids.txt\")\n",
    "error_401_file = os.path.join(data_dir, \"error_401_ids.txt\")\n",
    "retry_file = os.path.join(data_dir, \"retry_ids.txt\")\n",
    "\n",
    "starting_steam_id = \"76561197979408421\"  # Kongzoola\n",
    "max_depth = 3\n",
    "max_threads = 6\n",
    "batch_size = 10\n",
    "api_call_count = 0\n",
    "api_limit = 100000  # Daily API call limit\n",
    "reset_time = datetime.now() + timedelta(days=1)\n",
    "\n",
    "#### FUNCTIONS ####\n",
    "\n",
    "def log_error(message):\n",
    "    with open(error_log_file, 'a') as f:\n",
    "        f.write(f\"{datetime.now().isoformat()} - {message}\\n\")\n",
    "\n",
    "def load_existing_ids(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return set(line.strip() for line in f)\n",
    "    return set()\n",
    "\n",
    "def append_to_file(file_path, steam_id):\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write(f\"{steam_id}\\n\")\n",
    "\n",
    "def make_api_call(func, max_retries=3, delay=5, **kwargs):\n",
    "    global api_call_count, reset_time\n",
    "    api_call_count += 1\n",
    "\n",
    "    if api_call_count > api_limit:\n",
    "        if datetime.now() >= reset_time:\n",
    "            api_call_count = 0\n",
    "            reset_time = datetime.now() + timedelta(days=1)\n",
    "        else:\n",
    "            print(\"API limit reached. Pausing until reset...\")\n",
    "            time_until_reset = (reset_time - datetime.now()).total_seconds()\n",
    "            time.sleep(time_until_reset)\n",
    "            api_call_count = 0\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func(**kwargs)\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                time.sleep(delay * (2 ** attempt))\n",
    "            else:\n",
    "                log_error(f\"API call failed: {e}\")\n",
    "                break\n",
    "    log_error(f\"Max retries exceeded for API call.\")\n",
    "    return None\n",
    "\n",
    "def calculate_friendship_duration(friend_since):\n",
    "    friend_since_date = datetime.fromtimestamp(friend_since)\n",
    "    duration = datetime.now() - friend_since_date\n",
    "    return {\n",
    "        \"friend_since\": friend_since_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"friendship_duration_days\": duration.days\n",
    "    }\n",
    "\n",
    "def get_player_summaries_in_batches(steam_ids):\n",
    "    summaries = []\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(steam_ids), batch_size):\n",
    "        batch = steam_ids[i:i + batch_size]\n",
    "        try:\n",
    "            summary_data = make_api_call(api.ISteamUser.GetPlayerSummaries, steamids=\",\".join(batch))\n",
    "            if summary_data:\n",
    "                summaries.extend(summary_data.get(\"response\", {}).get(\"players\", []))\n",
    "        except Exception as e:\n",
    "            log_error(f\"Error in player summary batch {i // batch_size + 1}: {e}\")\n",
    "    return summaries\n",
    "\n",
    "def collect_player_data(steam_id):\n",
    "    global successful_ids, error_401_ids, retry_ids\n",
    "\n",
    "    if steam_id in successful_ids:\n",
    "        return None  # Skip already successful IDs\n",
    "\n",
    "    player_data = {\"steam_id\": steam_id, \"data\": {}}\n",
    "    try:\n",
    "        player_summaries = get_player_summaries_in_batches([steam_id])\n",
    "        if player_summaries:\n",
    "            player_data[\"data\"][\"player_info\"] = player_summaries[0]\n",
    "\n",
    "        owned_games = make_api_call(\n",
    "            api.IPlayerService.GetOwnedGames,\n",
    "            steamid=steam_id, include_appinfo=True,\n",
    "            include_played_free_games=True,\n",
    "            appids_filter=[]\n",
    "        )\n",
    "        if owned_games:\n",
    "            player_data[\"data\"][\"owned_games\"] = owned_games\n",
    "\n",
    "        friends = make_api_call(api.ISteamUser.GetFriendList, steamid=steam_id)\n",
    "        if friends and 'friendslist' in friends:\n",
    "            friends_data = []\n",
    "            for friend in friends['friendslist']['friends']:\n",
    "                friend_id = friend['steamid']\n",
    "                friend_since = friend.get('friend_since')\n",
    "                if friend_since:\n",
    "                    friends_data.append({\n",
    "                        \"friend_id\": friend_id,\n",
    "                        **calculate_friendship_duration(friend_since)\n",
    "                    })\n",
    "            player_data[\"data\"][\"friends\"] = friends_data\n",
    "\n",
    "        append_to_file(successful_file, steam_id)\n",
    "        successful_ids.add(steam_id)\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        if \"401\" in error_message:\n",
    "            append_to_file(error_401_file, steam_id)\n",
    "            error_401_ids.add(steam_id)\n",
    "        else:\n",
    "            append_to_file(retry_file, steam_id)\n",
    "            retry_ids.add(steam_id)\n",
    "        log_error(f\"Error processing Steam ID {steam_id}: {e}\")\n",
    "\n",
    "    return player_data\n",
    "\n",
    "def gather_steam_ids(steam_id, max_depth):\n",
    "    visited_ids = set([steam_id])\n",
    "    queue = deque([(steam_id, 0)])  # Store (steam_id, current_depth)\n",
    "\n",
    "    while queue:\n",
    "        current_id, depth = queue.popleft()\n",
    "        if depth >= max_depth:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            friends = make_api_call(api.ISteamUser.GetFriendList, steamid=current_id)\n",
    "            if friends and 'friendslist' in friends:\n",
    "                for friend in friends['friendslist']['friends']:\n",
    "                    friend_id = friend['steamid']\n",
    "                    if friend_id not in visited_ids:\n",
    "                        visited_ids.add(friend_id)\n",
    "                        queue.append((friend_id, depth + 1))\n",
    "        except Exception as e:\n",
    "            log_error(f\"Error retrieving friends for {current_id}: {e}\")\n",
    "\n",
    "    return visited_ids\n",
    "\n",
    "def process_players_in_batches(steam_ids):\n",
    "    global batch_counter, starting_steam_id\n",
    "    player_batch = []\n",
    "    with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "        futures = {executor.submit(collect_player_data, steam_id): steam_id for steam_id in steam_ids}\n",
    "\n",
    "        with tqdm(total=len(steam_ids), desc=\"Processing players\", unit=\"player\", dynamic_ncols=True) as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                steam_id = futures[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        player_batch.append(result)\n",
    "                except Exception as e:\n",
    "                    log_error(f\"Error processing Steam ID {steam_id}: {e}\")\n",
    "                pbar.update(1)\n",
    "\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump({\"players\": player_batch}, f, indent=4)\n",
    "\n",
    "#### MAIN SCRIPT ####\n",
    "\n",
    "successful_ids = load_existing_ids(successful_file)\n",
    "error_401_ids = load_existing_ids(error_401_file)\n",
    "retry_ids = load_existing_ids(retry_file)\n",
    "\n",
    "all_steam_ids = gather_steam_ids(starting_steam_id, max_depth)\n",
    "new_steam_ids = list(all_steam_ids - successful_ids - error_401_ids - retry_ids)\n",
    "\n",
    "process_players_in_batches(new_steam_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steam Data Processing Script in R\n",
    "\n",
    "#### Extracting Relevant Data from BYU Steam Dataset\n",
    "\n",
    "This script processes Steam data from the aquired BYU dataset using R. It performs the following tasks:\n",
    "\n",
    "1. **Load Necessary Libraries**: Loads required libraries such as `data.table` and `tcltk`.\n",
    "2. **File Path Selection**: Provides functions to select input and output file paths using a graphical interface.\n",
    "3. **Define Variables**: Sets definable variables such as chunk size and output filename.\n",
    "4. **Verify File Paths**: Checks if the input file exists and verifies the selected columns.\n",
    "5. **Process File in Chunks**: Reads the input file in chunks to save memory, processes each chunk and aggregates the game genre data into a single comma-separated \"Genres\" column\n",
    "6. **Write Output File**: Writes the processed data to an output file in TSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "library(data.table)\n",
    "library(tcltk)\n",
    "\n",
    "# Function to get file path for input or output\n",
    "get_file_path <- function(type = \"input\") {\n",
    "  if (type == \"input\") {\n",
    "    # Select an input file\n",
    "    input_file <- tk_choose.files(caption = \"Select an input file\", filter = matrix(c(\"CSV Files\", \"*.csv\"), ncol = 2))\n",
    "    if (length(input_file) == 0 || input_file == \"\") {\n",
    "      stop(\"No input file selected. Exiting...\")\n",
    "    }\n",
    "    return(input_file)\n",
    "  } else if (type == \"output\") {\n",
    "    # Select an output directory\n",
    "    output_dir <- tclvalue(tkchooseDirectory())\n",
    "    if (output_dir == \"\") {\n",
    "      stop(\"No output directory selected. Exiting...\")\n",
    "    }\n",
    "    return(output_dir)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Definable variables\n",
    "chunk_size <- 1e6  # Number of rows to read at a time\n",
    "output_filename <- \"steam_game_subset_2.tsv\"\n",
    "\n",
    "# Input/Output Variables\n",
    "input_file <- get_file_path(\"input\")\n",
    "output_dir <- get_file_path(\"output\")\n",
    "output_file <- file.path(output_dir, output_filename)\n",
    "\n",
    "# Verify file paths\n",
    "cat(\"Input file path: \", input_file, \"\\n\")\n",
    "cat(\"Output file path: \", output_file, \"\\n\")\n",
    "\n",
    "# Check if the input file exists\n",
    "if (!file.exists(input_file)) {\n",
    "  stop(\"Input file does not exist: \", input_file)\n",
    "}\n",
    "\n",
    "# Define the columns to keep (names without quotation marks)\n",
    "columns_to_keep <- c(\n",
    "  \"steamid\", \"personaname\", \"appid\", \"Title\", \"user_loccountrycode\", \n",
    "  \"number_of_friends\", \"number_of_groups\"\n",
    ")\n",
    "\n",
    "# Define genre columns (the ones that will contain 1s or 0s)\n",
    "genre_columns <- c(\n",
    "  \"Game_Genre_Action\", \"Game_Genre_Free_to_Play\", \"Game_Genre_Strategy\", \"Game_Genre_Adventure\", \n",
    "  \"Game_Genre_Indie\", \"Game_Genre_RPG\", \"Game_Genre_Animation_Modeling\", \n",
    "  \"Game_Genre_Video_Production\", \"Game_Genre_Casual\", \"Game_Genre_Simulation\", \n",
    "  \"Game_Genre_Racing\", \"Game_Genre_Massively_Multiplayer\", \"Game_Genre_Sports\", \n",
    "  \"Game_Genre_Early_Access\", \"Game_Genre_Photo_Editing\", \"Game_Genre_Utilities\", \n",
    "  \"Game_Genre_Design_Illustration\", \"Game_Genre_Education\", \n",
    "  \"Game_Genre_Software_Training\", \"Game_Genre_Web_Publishing\", \n",
    "  \"Game_Genre_Audio_Production\", \"Game_Genre_Accounting\"\n",
    ")\n",
    "\n",
    "# Read the header to get column names\n",
    "header <- fread(input_file, sep = \";\", nrows = 0)\n",
    "col_names <- gsub('\"', \"\", names(header))  # Remove any quotation marks\n",
    "setnames(header, col_names)  # Normalize column names\n",
    "\n",
    "# Verify the desired columns exist\n",
    "missing_columns <- setdiff(columns_to_keep, col_names)\n",
    "if (length(missing_columns) > 0) {\n",
    "  stop(\"The following columns are missing in the input file: \", paste(missing_columns, collapse = \", \"))\n",
    "}\n",
    "\n",
    "# Define the output columns\n",
    "output_columns <- c(columns_to_keep, \"Genres\")\n",
    "\n",
    "# Process the file in chunks\n",
    "chunk_start <- 1   # Starting row for the first chunk\n",
    "chunk_count <- 0\n",
    "\n",
    "while (TRUE) {\n",
    "  # Read the next chunk\n",
    "  chunk <- fread(\n",
    "    input = input_file,\n",
    "    sep = \";\",\n",
    "    header = FALSE,            \n",
    "    skip = chunk_start,        # Skip rows already processed\n",
    "    nrows = chunk_size,        # Read up to chunk_size rows\n",
    "    col.names = col_names      # Use consistent column names\n",
    "  )\n",
    "  \n",
    "  # Break the loop if no rows were read\n",
    "  if (nrow(chunk) == 0) break\n",
    "  \n",
    "  # Create the \"Genres\" column by checking which genre columns are 1\n",
    "  chunk[, Genres := apply(chunk[, ..genre_columns], 1, function(row) {\n",
    "    genres <- names(row)[row == 1]\n",
    "    if (length(genres) > 0) {\n",
    "      # Remove \"Game_Genre_\" from each genre name\n",
    "      cleaned_genres <- gsub(\"Game_Genre_\", \"\", genres)\n",
    "      return(paste(cleaned_genres, collapse = \", \"))\n",
    "    } else {\n",
    "      return(NA)  # No genres selected\n",
    "    }\n",
    "  })]\n",
    "  \n",
    "  # Ensure the chunk includes \"Genres\" before writing (do not add \"Genres\" again in the header)\n",
    "  chunk <- chunk[, c(columns_to_keep, \"Genres\"), with = FALSE]\n",
    "  \n",
    "  # If it's the first chunk, write the header\n",
    "  if (chunk_count == 0) {\n",
    "    fwrite(chunk, file = output_file, sep = \"\\t\", col.names = TRUE)\n",
    "  } else {\n",
    "    # For subsequent chunks, do not include the header again\n",
    "    fwrite(chunk, file = output_file, append = TRUE, sep = \"\\t\", col.names = FALSE)\n",
    "  }\n",
    "  \n",
    "  # Update the start of the next chunk\n",
    "  chunk_start <- chunk_start + chunk_size\n",
    "  chunk_count <- chunk_count + 1\n",
    "  print(chunk_start)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting SteamID, User Location and Number of Friends\n",
    "\n",
    "This script processes the generated TSV datafile.\n",
    "\n",
    "1. **Load Necessary Libraries**: Loads `data.table`.\n",
    "2. **Define Variables**: Sets definable variables for input and putput filenames, columns to extract and filter parameters.\n",
    "3. **Function to Process File in Chunks**: Reads the input file in chunks, processes each chunk and removes duplicates\n",
    "4. **Write Output File**: Writes the selected and deduplicated data to the output file in TSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(data.table)\n",
    "\n",
    "### VARIABLES ###\n",
    "data_input_file <- \"E:/OneDrive - University of Utah/U of U/Graduate School/COMP5690/Project/Steam_data_sources/Steam_Games_BYU/steam_game_subset.tsv\"\n",
    "data_output_file <- \"steamid_loc_friends.tsv\"\n",
    "selected_columns <- c(\"steamid\", \"personaname\", \"user_loccountrycode\", \"number_of_friends\")\n",
    "unique_id_column <- \"steamid\"  # Column to identify unique entries\n",
    "batch_size <- 1e6\n",
    "\n",
    "### FUNCTIONS ###\n",
    "process_large_tsv <- function(input_file, output_file, selected_columns, id_column, batch_size = 10000) {\n",
    "  # Open the input file for reading\n",
    "  con <- file(input_file, open = \"r\")\n",
    "  \n",
    "  # Read and validate the header\n",
    "  header <- strsplit(readLines(con, n = 1), \"\\t\")[[1]]\n",
    "  col_indices <- which(header %in% selected_columns)\n",
    "  id_col_index <- which(header == id_column)\n",
    "  \n",
    "  if (!all(selected_columns %in% header)) {\n",
    "    stop(\"Error: Some selected columns not found in the header. Missing columns: \", \n",
    "         paste(setdiff(selected_columns, header), collapse = \", \"))\n",
    "  }\n",
    "  \n",
    "  if (!(id_column %in% header)) {\n",
    "    stop(\"Error: The id_column '\", id_column, \"' is not found in the header.\")\n",
    "  }\n",
    "  \n",
    "  # Write the header to the output file\n",
    "  fwrite(\n",
    "    data.table(t(header[col_indices])),\n",
    "    file = output_file,\n",
    "    sep = \"\\t\",\n",
    "    col.names = FALSE,\n",
    "    quote = FALSE\n",
    "  )\n",
    "  \n",
    "  # Initialize a data.table to track unique entries\n",
    "  unique_ids <- data.table()\n",
    "  \n",
    "  # Process the file in batches\n",
    "  while (length(lines <- readLines(con, n = batch_size)) > 0) {\n",
    "    cat(\"Processing batch of size:\", length(lines), \"\\n\")\n",
    "    \n",
    "    # Split lines into columns and create a data.table\n",
    "    batch_dt <- as.data.table(do.call(rbind, strsplit(lines, \"\\t\")))\n",
    "    \n",
    "    # Handle cases where row lengths may not match the header\n",
    "    if (ncol(batch_dt) != length(header)) {\n",
    "      cat(\"Warning: Row length mismatch detected. Skipping malformed rows.\\n\")\n",
    "      batch_dt <- batch_dt[, seq_along(header), with = FALSE]\n",
    "    }\n",
    "    \n",
    "    setnames(batch_dt, header)\n",
    "    \n",
    "    # Select relevant columns\n",
    "    batch_dt <- batch_dt[, ..col_indices]\n",
    "    \n",
    "    # Remove rows with empty or NA values\n",
    "    batch_dt <- batch_dt[!apply(batch_dt, 1, function(row) any(row == \"\" | is.na(row)))]\n",
    "    \n",
    "    # Deduplicate within the batch and exclude already processed IDs\n",
    "    batch_dt <- unique(batch_dt)\n",
    "    batch_dt <- batch_dt[!batch_dt[[id_column]] %in% unique_ids[[id_column]]]\n",
    "    \n",
    "    # Update the unique ID tracking table\n",
    "    if (nrow(batch_dt) > 0) {\n",
    "      unique_ids <- rbindlist(list(unique_ids, batch_dt[, .(get(id_column))]), use.names = FALSE)\n",
    "      \n",
    "      # Write the processed batch to the output file\n",
    "      fwrite(batch_dt, output_file, sep = \"\\t\", append = TRUE, col.names = FALSE)\n",
    "      cat(\"Wrote\", nrow(batch_dt), \"unique rows to the output file.\\n\")\n",
    "    } else {\n",
    "      cat(\"No new unique entries found in this batch.\\n\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  close(con)  # Close the input file connection\n",
    "  cat(\"Processing complete. Output saved to:\", output_file, \"\\n\")\n",
    "}\n",
    "\n",
    "### MAIN ###\n",
    "process_large_tsv(data_input_file, data_output_file, selected_columns, unique_id_column, batch_size)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
